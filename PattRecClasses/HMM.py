import numpy as np
from .DiscreteD import DiscreteD
from .GaussD import GaussD
from .MarkovChain import MarkovChain
from .GMM import GMM
from ipywidgets import IntProgress
from IPython.display import display
import multiprocessing

class HMM:
		"""
		HMM - class for Hidden Markov Models, representing
		statistical properties of random sequences.
		Each sample in the sequence is a scalar or vector, with fixed DataSize.
		
		Several HMM objects may be collected in a single multidimensional array.
		
		A HMM represents a random sequence(X1,X2,....Xt,...),
		where each element Xt can be a scalar or column vector.
		The statistical dependence along the (time) sequence is described
		entirely by a discrete Markov chain.
		
		A HMM consists of two sub-objects:
		1: a State Sequence Generator of type MarkovChain
		2: an array of output probability distributions, one for each state
		
		All states must have the same class of output distribution,
		such as GaussD, GaussMixD, or DiscreteD, etc.,
		and the set of distributions is represented by an object array of that class,
		although this is NOT required by general HMM theory.
		
		All output distributions must have identical DataSize property values.
		
		Any HMM output sequence X(t) is determined by a hidden state sequence S(t)
		generated by an internal Markov chain.
		
		The array of output probability distributions, with one element for each state,
		determines the conditional probability (density) P[X(t) | S(t)].
		Given S(t), each X(t) is independent of all other X(:).
		
		
		References:
		Leijon, A. (20xx) Pattern Recognition. KTH, Stockholm.
		Rabiner, L. R. (1989) A tutorial on hidden Markov models
			and selected applications in speech recognition.
			Proc IEEE 77, 257-286.
		
		"""
		def __init__(self, mc, distributions):

				self.stateGen = mc
				self.outputDistr = distributions

				self.nStates = mc.nStates
				self.dataSize = distributions[0].dataSize
		
		def rand(self, nSamples):
				"""
				[X,S]=rand(self,nSamples); generates a random sequence of data
				from a given Hidden Markov Model.
				
				Input:
				nSamples=  maximum no of output samples (scalars or column vectors)
				
				Result:
				X= matrix or row vector with output data samples
				S= row vector with corresponding integer state values
					obtained from the self.StateGen component.
					nS= length(S) == size(X,2)= number of output samples.
					If the StateGen can generate infinite-duration sequences,
							nS == nSamples
					If the StateGen is a finite-duration MarkovChain,
							nS <= nSamples
				"""
				
				#*** Insert your own code here and remove the following error message

				# Retrieve the state sequence with the number of samples specified in the input to the method
				#   self.stateGen 						-->			retrieves the MarkovChain provided to the HMM object when it was constructed
				#   self.stateGen.rand(...)		-->			apply the rand method to the MarkovChain object
				#   this will return a np.array with a state sequence (if the MarkovChain is infinite "len(S) = nSamples", otherwise "len(S) <= nSamples")
				S = self.stateGen.rand(nSamples)

				# Generate random samples from the distribution corresponding to each state in the state sequence
				#		[ self.outputDistr[i].rand(1) for i in S ]		-->			for each state, retrieve the corresponding distribution and apply the rand method to retrieve a sample (scalar or vector)
				#				the result is a list of np.array(s)
				#   np.asarray(...) or np.array(...) will convert the list to an np.array
				X = np.asarray([self.outputDistr[i].rand(1) for i in S])
				# The previous line of code instantiated a matrix with dimensions: (dim1 <= nSamples , dim2 = self.dataSize , dim3 = 1)
				#   i.e. the samples for each state will be allocated along the columns rather than the rows.
				# 	in order to fulfill the specifications we return the transpose of X (X.T) and retrieve the inner matrix (X.T[0])
				# 		by doing so the resulting matrix will have dimension: (dim1 = self.dataSize, dim2 <= nSamples)
				#			where each column "t" corresponds to a sample drawn from the corresponding state S_{t}
				
				return X.T, S

		def ab(self, pX, scale=True):
			p, scaled = self.prob(pX)
			if not scale:
				scaled=pX
			alpha, c = self.stateGen.forward(scaled)
			beta = self.stateGen.backwards(scaled, c)
			return alpha, c, beta

		def alphahat(self, pX,scale = True):
			p, scaled = self.prob(pX) # this does not make sense yet
			if not scale:
				scaled = p
			alpha, c = self.stateGen.forward(scaled)
			return alpha, c


		def betahat(self, pX,scale = True):
			p, scaled = self.prob(pX) # this does not make sense yet
			if not scale:
				scaled = p
			
			_, c = self.alphahat(pX)
			beta = self.stateGen.backwards(scaled, c)
				
			return beta

		def prob(self, X):
			res = np.array([[b.likelihood(x) for x in X.T] for b in self.outputDistr])
			
			scaled = np.array([value/(np.amax(value)) for value in res.T]).T
			return res, scaled


		def viterbi(self):
				pass

		def train(self):
				pass

		def stateEntropyRate(self):
				pass

		def setStationary(self):
				pass

		def classifier(self, x):
				# compute the gauss logprob method for each GaussD object in the outputDistr
				#pX = np.array([gauss.logprob(x) for gauss in self.outputDistr])
				pX = np.array([np.log(gauss.likelihood(x)+1e-19) for gauss in self.outputDistr])
				# call to the forward algorithm with the exponential of the scaled conditional probabilities
				_, c = self.stateGen.forward(np.exp(pX))
				logP = np.sum(np.log(c))		# compute the log-probability calculation

				return logP
		
		
		def logprob(self,x):
			res, scaled = self.prob(x)
			return np.log(res), np.log(scaled)
		
		def adaptStart(self):
				pass

		def adaptSet(self):
				pass

		def adaptAccum(self):
				pass



		def baum_welch(self, obs, niter, uselog = False):
			
			for item in np.arange(niter):
				print(item)
				print('calcabc')
				alphahats, betahats, cs = self.calcabc(obs)
				print('calcgammas')
				gammas = self.calcgammas(alphahats, betahats, cs, obs, uselog)
				print('calcinit')
				newpi = self.calcinit(gammas, uselog)
				print('calcxi')
				xibar = self.calcxi(alphahats, betahats, cs, obs,uselog)
				print('updating matrices')
				if uselog:
					xibar = np.exp(xibar)
					gammas = np.exp(gammas)
				newA = np.array([i/np.sum(i) for i in xibar])
				self.stateGen.q = newpi
				self.stateGen.A = newA
				ws = np.array([b.get_weights() for b in self.outputDistr])
				
				gs = np.array([[self.outputDistr[b].get_pdfs(r) for b in np.arange(len(self.outputDistr))] for r in obs])	
				wsgs = np.einsum('jk,ijkl->ijkl', ws, gs) #x, states, features, time
				norm = 1/np.sum(wsgs, axis=(2))
				gamma_imt = gammas*norm
				gamma_imt = np.einsum('ijl,ijkl->ijkl', gamma_imt, wsgs)
				
				new_w = np.einsum('j,jk->jk',1/np.sum(gamma_imt, axis=(0,2,3)), np.sum(gamma_imt,axis=(0,3))) # it appears that the values of weights do not update: debus later (this might be due to the fact that i do not update the output distributions)

				num = np.einsum('ijkl,ikl->ijkl', gamma_imt, obs)# i think the problem its here: the mean does not have the same size as the observed vector 
				den = 1/np.sum(gamma_imt,axis=(0,3))
				num = np.sum(num, axis=(0,3))
				new_mu = np.einsum('jk,jk->jk', num,den)

				res = np.zeros(shape=(obs.shape[0],new_mu.shape[0],new_mu.shape[1],obs.shape[-1])) # hard coded: change it later
				for a in np.arange(obs.shape[0]):
					for b in np.arange(new_mu.shape[0]):
						for c in np.arange(new_mu.shape[1]):
							for d in np.arange(obs.shape[-1]):
								res[a,b,c,d] = obs[a,c,d] - new_mu[b,c] #2x5x13x66
				#res[:] = temp_res
				res = np.sum(res, axis=(0))
				gamma_imt = np.sum(gamma_imt,axis=(0))
				rhs=gamma_imt*(res**2)
				rhs = np.sum(rhs, axis=(-1))
				new_cov = rhs/np.sum(gamma_imt, axis=(0,-1))
				self.outputDistr = np.array([GMM(new_mu[i], new_cov[i], weights=new_w[i]) for i in np.arange(new_w.shape[0]) ])
				# let us find a way to update the variables in the outputdistribution
				# in order to update weights and means, it is necessary to compute gammaimt #page 160
				# calculate the value of weight for every
				# update weights
				# update means
				# update covariance matrix




			return newpi, newA, [new_w, new_mu, new_cov]



		"""
		lets try to adapt the code provided by the TAs
		"""
		def calcabc(self, obs):
			n_states = self.stateGen.A.shape[0]
			alphahats = np.zeros(shape=(obs.shape[0],n_states, obs.shape[-1]))
			betahats = np.zeros(shape=(obs.shape[0],n_states, obs.shape[-1]))
			cs = np.zeros(shape =(obs.shape[0],obs.shape[-1]+self.stateGen.is_finite))
			pool = multiprocessing.Pool()
			res = np.array(pool.map(self.ab,obs), dtype=object)
			for rows in np.arange(res.shape[0]):
				alphahats[rows] = res[rows,0]
				cs[rows] = res[rows,1]
				betahats[rows] = res[rows,2]
            #forward_pool = multiprocessing.Pool()
			#forward = np.array(forward_pool.map(self.alphahat, obs),dtype=object)
			#for rows in np.arange(forward.shape[0]):
			#	alphahats[rows] = forward[rows,0]
			#	cs[rows] = forward[rows,1]
			#backwards_pool = multiprocessing.Pool()
			#betahats = np.array(backwards_pool.map(self.betahat, obs))
			
			"""
			for i in np.arange(obs.shape[0]):
				f.value = i
				x = multiprocessing.Process(target=self.alphahat, args=(obs[i],))
				y = multiprocessing.Process(target=self.betahat, args=(obs[i],))
				alpha_threads.append(x)
				beta_threads.append(y)
				x.start()
				y.start()
			for index, thread in enumerate(alpha_threads):
				alphahats[index,:,:], cs[index,:] = thread[index].join()
			
			for index, thread in enumerate(beta_threads):
				betahats[index,:,:] = thread[index].join()
			#for i in np.arange(obs.shape[0]):
			#	alphahats[i,:,:], cs[i,:] = self.alphahat(obs[i])
			#	betahats[i,:,:] = self.betahat(obs[i])	
			"""
			return alphahats, betahats, cs


		def calcgammas(self, alphahats, betahats, cs, obs, uselog=False):
			n_states = self.stateGen.A.shape[0]
			gammas = np.zeros(shape=(obs.shape[0],n_states, obs.shape[-1]))
			for i in np.arange(obs.shape[0]):
				if uselog:
					gammas[i,:,:] = np.log(alphahats[i]) + np.log(betahats[i]) + np.log(cs[i])
				else:
					gammas[i,:,:] = alphahats[i]*betahats[i]*cs[i,:-1]
			return gammas

		def calcinit(self, gammas, uselog=False):
			
			if uselog:
				return np.mean(np.exp(gammas[:,:,0]), axis=0)
			else:
				return np.mean(gammas[:,:,0], axis=0)

		def calcxi(self, alphahats, betahats, cs, obs, uselog=False):

			
			A_dim = self.stateGen.A.shape # rows, cols
			n_states = A_dim[0]
			A = self.stateGen.A[:,:-1]
			A = A.reshape(A_dim[0],A_dim[0],1) # we consider a square matrix (we will take care of the ext condition late)
			xi = np.zeros(shape=(obs.shape[0], A_dim[0], A_dim[1], obs.shape[2]-1+self.stateGen.is_finite)) #R, N, N, Tr-1

			prob_pool = multiprocessing.Pool()
			if uselog:
				prob_res = np.array(prob_pool.map(self.logprob, obs))
			else:
				prob_res = np.array(prob_pool.map(self.prob, obs))
			
			p = prob_res[:,0,:,:]
			scaled = prob_res[:,1,:,:]
			xi[:,:,:-1,:-1] = A*np.einsum('rij,rkj->rikj',alphahats[:,:n_states,:-1],scaled[:,:n_states,1:]*betahats[:,:n_states,1:])[:]
			if self.stateGen.is_finite:
				if uselog:
					xi[:,:,-1,-1] = np.log(alphahats[:,:,-1]) + np.log(betahats[:,:,-1]) + np.log(cs[:,-1])
				else:
					print(xi[:,:,-1,-1].shape, alphahats[:,:,-1].shape,betahats[:,:,-1].shape, cs[:,-1].shape)
					xi[:,:,-1,-1] = np.einsum('ij,i->ij',alphahats[:,:,-1]*betahats[:,:,-1],cs[:,-1])
			#xi[:,:,:-1,:-1] = np.einsum('ik,rikj->rikj', A,xi[:,:,:-1,:-1])
			#for r in np.arange(obs.shape[0]):
				#p, scaled = self.prob(obs[r], self.outputDistr)
				
				#if uselog:
					#p, scaled = self.logprob(obs[r],self.outputDistr)
				#xi[r,:,:-1,:-1] = A*np.einsum('ij,kj->ikj',alphahats[r,:n_states,:-1],(scaled[r,:n_states,1:]*betahats[r,:n_states,1:]))

				#if self.stateGen.is_finite:
				#	if uselog:
				#		xi[r,:,-1,-1] = np.log(alphahats[r,:,-1]) + np.log(betahats[r,:,-1])+np.log(cs[r,-1])
				#	else:
				#		xi[r,:,-1,-1] = alphahats[r,:,-1]*betahats[r,:,-1]*cs[r,-1]
			
			if uselog:
				xi = np.exp(xi)
			xibars = np.sum(xi, axis=-1)
			xibar = np.sum(xibars, axis=0)
			return xibar
				

